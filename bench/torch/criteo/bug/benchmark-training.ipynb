{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Benchmark NVTabular data loader\n",
    "We are interested to benchmark the NVTabular data loader and compare its performance to the TensorFlow \"native\" data loader based on tf.records. In [benchmark-preprocess.ipynb](???), we preprocess the dataset, ready to use for NVTabular data loader (parquet) and TensorFlow native data loader (tf.records). In this notebook, we will train a neural network in TensorFlow using either data loader and measure the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.4.0\n",
      "  Downloading tensorflow_gpu-2.4.0-cp38-cp38-manylinux2010_x86_64.whl (394.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.8 MB 43 kB/s s eta 0:00:01     |████████████████████████████    | 346.1 MB 84.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /conda/envs/rapids/lib/python3.8/site-packages (from tensorflow-gpu==2.4.0) (3.15.5)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 64.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /conda/envs/rapids/lib/python3.8/site-packages (from tensorflow-gpu==2.4.0) (0.36.2)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /conda/envs/rapids/lib/python3.8/site-packages (from tensorflow-gpu==2.4.0) (1.15.0)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 62.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 70.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 62.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting h5py~=2.10.0\n",
      "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 66.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /conda/envs/rapids/lib/python3.8/site-packages (from tensorflow-gpu==2.4.0) (3.7.4.3)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 76.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 66.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /conda/envs/rapids/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.25.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /conda/envs/rapids/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (49.6.0.post20210108)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.28.0-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 72.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 66.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 83.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /conda/envs/rapids/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.26.3)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 74.2 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=36ddf6a20d2f9ab285ba363390250f8d31dac5f00f1428f26cefecd44be68a9e\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19553 sha256=91255199f23e5e073ea016a8c526555fdbf62f2371a136a03f2057f5af863f5a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-gpu\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.1\n",
      "    Uninstalling numpy-1.20.1:\n",
      "      Successfully uninstalled numpy-1.20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.2.7 requires torchvision<0.9,>=0.8, which is not installed.\n",
      "cudf 0.18.0 requires cython, which is not installed.\n",
      "fastai 2.2.7 requires spacy<3, but you have spacy 3.0.3 which is incompatible.\n",
      "fastai 2.2.7 requires torch<1.8,>=1.7.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.1 flatbuffers-1.12 gast-0.3.3 google-auth-1.28.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-estimator-2.4.0 tensorflow-gpu-2.4.0 termcolor-1.1.0 werkzeug-1.0.1 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.20.1\n",
      "  Downloading numpy-1.20.1-cp38-cp38-manylinux2010_x86_64.whl (15.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.4 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.2.7 requires torchvision<0.9,>=0.8, which is not installed.\n",
      "cudf 0.18.0 requires cython, which is not installed.\n",
      "tensorflow-gpu 2.4.0 requires numpy~=1.19.2, but you have numpy 1.20.1 which is incompatible.\n",
      "fastai 2.2.7 requires spacy<3, but you have spacy 3.0.3 which is incompatible.\n",
      "fastai 2.2.7 requires torch<1.8,>=1.7.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run single GPU version and set only one visible device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import nvtabular as nvt\n",
    "\n",
    "from time import time\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define multiple helper functions.<br><br>\n",
    "*get_dataloader* returns the NVTabular data loader or TensorFlow native data loader, depending on dl_type<br>\n",
    "*get_model* returns a standard TensorFlow model<br>\n",
    "*make_tf_dataset* is a helper function to initalize the TensorFlow data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function\n",
    "\n",
    "def get_dataloader(dl_type='NVTabular', columns=[], HASH=False):\n",
    "    if dl_type=='NVTabular':\n",
    "        if HASH:\n",
    "            workflow, columns = make_feature_column_workflow(columns, LABEL_COLUMNS[0])\n",
    "        train_dataset_tf = KerasSequenceLoader(\n",
    "            output_train_dir, # you could also use a glob pattern\n",
    "            batch_size=BATCH_SIZE,\n",
    "            label_names=LABEL_COLUMNS,\n",
    "            cat_names=CATEGORICAL_COLUMNS,\n",
    "            cont_names=CONTINUOUS_COLUMNS,\n",
    "            engine='parquet',\n",
    "            shuffle=True,\n",
    "            buffer_size=0.06, # how many batches to load at once\n",
    "            parts_per_chunk=PARTS_PER_CHUNK\n",
    "        )\n",
    "        valid_dataset_tf = KerasSequenceLoader(\n",
    "            output_valid_dir, # you could also use a glob pattern\n",
    "            batch_size=BATCH_SIZE,\n",
    "            label_names=LABEL_COLUMNS,\n",
    "            cat_names = CATEGORICAL_COLUMNS,\n",
    "            cont_names=CONTINUOUS_COLUMNS,\n",
    "            engine='parquet',\n",
    "            shuffle=False,\n",
    "            buffer_size=0.06,\n",
    "            parts_per_chunk=PARTS_PER_CHUNK\n",
    "        )\n",
    "        if HASH:\n",
    "            train_dataset_tf.map(workflow)\n",
    "            valid_dataset_tf.map(workflow)\n",
    "    if dl_type=='TensorFlow':\n",
    "        train_dataset_tf = make_tf_dataset(TFRECORDS_TRAIN, columns)\n",
    "        valid_dataset_tf = make_tf_dataset(TFRECORDS_VALID, columns)\n",
    "    return(train_dataset_tf, valid_dataset_tf, columns)\n",
    "\n",
    "def get_model(hidden_dims, inputs, features, dl_type):\n",
    "    if dl_type=='NVTabular':\n",
    "        dense_layer = layers.DenseFeatures(features)\n",
    "    if dl_type=='TensorFlow':\n",
    "        dense_layer = tf.keras.layers.DenseFeatures(features)\n",
    "    x = dense_layer(inputs)\n",
    "    \n",
    "    for hidden in hidden_dims:\n",
    "        x = tf.keras.layers.Dense(hidden, activation='relu')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    metrics = [tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\")]\n",
    "    model.compile('sgd', 'binary_crossentropy', metrics=metrics)\n",
    "    return(model)\n",
    "\n",
    "def make_tf_dataset(file_pattern, columns):\n",
    "    # get rid of embeddings for \"raw\" columns\n",
    "    columns = [getattr(col, \"categorical_column\", col) for col in columns]\n",
    "    # feature spec tells us how to parse tfrecords\n",
    "    # using FixedLenFeatures keeps from using sparse machinery,\n",
    "    # but obviously wouldn't extend to multi-hot categoricals\n",
    "    get_dtype = lambda col: getattr(col, \"dtype\", tf.int64)\n",
    "    feature_spec = {column.name: tf.io.FixedLenFeature((1,), get_dtype(column)) for column in columns}\n",
    "    feature_spec[LABEL_COLUMNS[0]] = tf.io.FixedLenFeature((1,), tf.int64)\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern,\n",
    "        BATCH_SIZE,\n",
    "        feature_spec,\n",
    "        label_key=LABEL_COLUMNS[0],\n",
    "        num_epochs=EPOCHS,\n",
    "        shuffle=True,\n",
    "        shuffle_buffer_size=BATCH_SIZE,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def log_textfile(filename, text, mode):\n",
    "    print(text)\n",
    "    f = open(filename, mode)\n",
    "    f.write(str(text) + str('\\n'))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we define functions to measure the performance.<br><br>\n",
    "*time_only_dl* measures the time for just iterating through the dataset for 1 epoch WITHOUT training a model<br>\n",
    "*time_training* measures the time for training a model for 1 epoch<br><br>\n",
    "Note, that 1 epoch is defined by a number of steps. Tf.records does not allow partical batches in tf.records, so we approximated one epoch by the number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_only_dl(dl, num_steps):\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    j= 0\n",
    "    bl_done = False\n",
    "    while not(bl_done) and i<num_steps:\n",
    "        for _, batch in enumerate(dl):\n",
    "            if i == num_steps:\n",
    "                bl_done = True\n",
    "                break\n",
    "            i+=1\n",
    "        j+=1\n",
    "    end = time.time()\n",
    "    return(end-start, i, j)\n",
    "\n",
    "def time_training(model, train_dataset_tf, steps):\n",
    "    start = time.time()\n",
    "    history = model.fit(train_dataset_tf, epochs=1)\n",
    "    end = time.time()\n",
    "    return(end-start, steps, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define which benchmark, we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMP = False\n",
    "DL_TYPES = ['NVTabular', 'TensorFlow']\n",
    "BENCHMARK_TYPES = ['time_only_dl', 'time_training', 'convergence_training_loss', 'convergence_val_loss']\n",
    "DL_TYPE = 'NVTabular'\n",
    "BENCHMARK_TYPE = 'time_training'\n",
    "HASH = False\n",
    "CPU = False\n",
    "\n",
    "if DL_TYPE not in DL_TYPES:\n",
    "    raise ValueError(DL_TYPE + ' is not supported.  Choose from ' + str(DL_TYPES))\n",
    "    \n",
    "if BENCHMARK_TYPE not in BENCHMARK_TYPES:\n",
    "    raise ValueError(BENCHMARK_TYPE + ' is not supported. Choose from ' + str(BENCHMARK_TYPES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the inpurt directory for the parquet and tf.records file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/raid/data/criteo/output/output/train/12.353c8050ed1f4702bde63bd697c9cc36.parquet',\n",
       "  '/raid/data/criteo/output/output/train/18.6e5ac8d8f49f46089cecceb6d044adda.parquet',\n",
       "  '/raid/data/criteo/output/output/train/14.98377a29380145999e7e1e0d4c75eb81.parquet',\n",
       "  '/raid/data/criteo/output/output/train/4.1907f580308044b6a32da25e9d3f1540.parquet',\n",
       "  '/raid/data/criteo/output/output/train/8.68a9f5f1feb64fe59fe160211b54afd4.parquet',\n",
       "  '/raid/data/criteo/output/output/train/1.8772a8717fae46df8b521cdd891df07a.parquet',\n",
       "  '/raid/data/criteo/output/output/train/16.7a1f69dd7d834844bbfb68e659bb362c.parquet',\n",
       "  '/raid/data/criteo/output/output/train/6.46cd6180dc42484199d895a2de02a1a6.parquet',\n",
       "  '/raid/data/criteo/output/output/train/2.1063c71d33e646fe8e6218bc2457c3cb.parquet',\n",
       "  '/raid/data/criteo/output/output/train/7.1a249ca9dbf44b8e9ca6b0ce740e3a88.parquet',\n",
       "  '/raid/data/criteo/output/output/train/0.cb895dbeefa14e65aa2e715b295ac9e7.parquet',\n",
       "  '/raid/data/criteo/output/output/train/5.c311b5980d704e93816fa7804e2a3857.parquet',\n",
       "  '/raid/data/criteo/output/output/train/9.74f0e8c3d56d4dcfb5612ad43d5b8adb.parquet',\n",
       "  '/raid/data/criteo/output/output/train/11.e174d5a532c94b69822fd863478c4e04.parquet',\n",
       "  '/raid/data/criteo/output/output/train/13.95d6f2a6766048088ee7ae81a79bc1bb.parquet',\n",
       "  '/raid/data/criteo/output/output/train/17.48205e924f50415d83009a48ff3ee19f.parquet',\n",
       "  '/raid/data/criteo/output/output/train/10.3aa47f4aa3144735a2b0f71dc88fa918.parquet',\n",
       "  '/raid/data/criteo/output/output/train/15.7be16af0b3fa47a4897b5a0ab595d7be.parquet',\n",
       "  '/raid/data/criteo/output/output/train/19.f79d7d5b19864ce69e7ff41131c87c09.parquet',\n",
       "  '/raid/data/criteo/output/output/train/3.9fe016cc81694cee9f05674324ae0dba.parquet'],\n",
       " ['/raid/data/criteo/output/output/valid/13.d4724e7f0834411cb46533f8f8c86034.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/4.c202f22751934899beaaeee483a5e9e2.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/1.3aca89e5106c4e7f8a06cc71440e0b6a.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/15.9303e2ee042c4146a0aaa6a5d4850a15.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/10.8c43c55cca2e425d9ce43148efaed85b.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/3.6e231834104c45a2a1012a7195788261.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/17.707601bf952f47279c7c1ba49259092b.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/14.bfd3a717ed6b44a8b4fcf366c67e3b65.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/7.4ae3f3bdf2514499b8a9ce04b5312797.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/19.44f0eba35c204affb20e55401decea13.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/0.f90e29f44474454ea6ad03a0bdfa9d74.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/16.efb3bd3869b44ee8bc6a4bbdc564cf54.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/6.ad8ec6d6447b48619afa548be23336a8.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/2.63899d36733c43f691b4eaafcc82621e.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/12.d613b4f1a9554ab3840e5c81acf06c71.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/5.5aae3277b269426ab6e8446eee287907.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/18.4e9d894e2ba744d688a1eaebaf39864f.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/8.1d840945b6664380afddc4f073037404.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/9.1317d2c0af2a49e8bdafb77eaccac160.parquet',\n",
       "  '/raid/data/criteo/output/output/valid/11.2b92bf253ebf43bd8b4537263b2b233b.parquet'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define some information about where to get our data\n",
    "OUTPUT_DIR = '/raid/data/criteo/output/'\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', OUTPUT_DIR + 'output') # where we'll save our procesed data to\n",
    "\n",
    "output_train_dir = glob.glob(os.path.join(OUTPUT_DATA_DIR, 'train/*.parquet'))\n",
    "output_valid_dir = glob.glob(os.path.join(OUTPUT_DATA_DIR, 'valid/*.parquet'))\n",
    "\n",
    "output_train_dir, output_valid_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some hyperparameters and network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for training the deep learning model\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 1024*64))      \n",
    "# Number of epochs (only for convergence_val_loss)\n",
    "EPOCHS = 1\n",
    "# Number of steps in training to collect train_loss (only for convergence_training_loss)\n",
    "TRAIN_STEPS = 20\n",
    "# Max. number of steps per epoch (tf.records allows only full batches)\n",
    "STEPS = int(150000000/BATCH_SIZE)\n",
    "# Number of units in hidden layer - length is number of hidden layers\n",
    "HIDDEN_DIMS = [1024, 1024, 1024, 1024]\n",
    "# Number of parts using in shuffling of NVTabular data loader\n",
    "PARTS_PER_CHUNK = int(os.environ.get('PARTS_PER_CHUNK', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the saved NVTabular workflow to extract the data schema and some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TABLE_SHAPES = {'C1': (7599500, 16),\n",
    " 'C10': (5345303, 16),\n",
    " 'C11': (561810, 16),\n",
    " 'C12': (242827, 16),\n",
    " 'C13': (11, 6),\n",
    " 'C14': (2209, 16),\n",
    " 'C15': (10616, 16),\n",
    " 'C16': (100, 16),\n",
    " 'C17': (4, 3),\n",
    " 'C18': (968, 16),\n",
    " 'C19': (15, 7),\n",
    " 'C2': (33521, 16),\n",
    " 'C20': (7838519, 16),\n",
    " 'C21': (2580502, 16),\n",
    " 'C22': (6878028, 16),\n",
    " 'C23': (298771, 16),\n",
    " 'C24': (11951, 16),\n",
    " 'C25': (97, 16),\n",
    " 'C26': (35, 12),\n",
    " 'C3': (17022, 16),\n",
    " 'C4': (7339, 16),\n",
    " 'C5': (20046, 16),\n",
    " 'C6': (4, 3),\n",
    " 'C7': (7068, 16),\n",
    " 'C8': (1377, 16),\n",
    " 'C9': (63, 16)}\n",
    "\n",
    "CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\n",
    "CATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\n",
    "LABEL_COLUMNS = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import TensorFlow and set *TF_MEMORY_ALLOCATION*, that TensorFlow will not reserve the full GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.feature_column import feature_column_v2 as fc\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"0.5\" # fraction of free memory\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader, KerasSequenceValidater\n",
    "from nvtabular.framework_utils.tensorflow import layers\n",
    "from tensorflow.python.feature_column import feature_column_v2 as fc\n",
    "\n",
    "from nvtabular.framework_utils.tensorflow import make_feature_column_workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the tf.keras.Input tensor and tf.feature_column s. A common technique is to use hashing with `tf.feature_column.categorical_column_with_hash_bucket` to reduce the dimensonality of the embedding tables. Optional we can add hash columns to our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "features = []\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    inputs[col] =  tf.keras.Input(\n",
    "        name=col,\n",
    "        dtype=tf.int32,\n",
    "        shape=(1,)\n",
    "    )\n",
    "    features.append(\n",
    "        tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\n",
    "                col, \n",
    "                EMBEDDING_TABLE_SHAPES[col][0]                    # Input dimension (vocab size)\n",
    "            ), EMBEDDING_TABLE_SHAPES[col][1]                     # Embedding output dimension\n",
    "        )\n",
    "    )\n",
    "for col in CONTINUOUS_COLUMNS:\n",
    "    inputs[col] =  tf.keras.Input(\n",
    "        name=col,\n",
    "        dtype=tf.float32,\n",
    "        shape=(1,)\n",
    "    )\n",
    "    features.append(\n",
    "        tf.feature_column.numeric_column(col, (1,))\n",
    "    )\n",
    "hash_postfix = 'nohash'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the data loader, depending on the data loader type DL_TYPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tf, valid_dataset_tf, features = get_dataloader(DL_TYPE, features, HASH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify to use mixed precision for the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AMP:\n",
    "    amp_postfix = 'amp'\n",
    "else:\n",
    "    amp_postfix = 'noamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2989/2989 [==============================] - 444s 148ms/step - loss: 0.1747 - auroc: 0.5743\n"
     ]
    }
   ],
   "source": [
    "logfilename = 'testtf.log'\n",
    "if BENCHMARK_TYPE=='time_training':\n",
    "    model = get_model(HIDDEN_DIMS, inputs, features, DL_TYPE)\n",
    "    run_time, num_steps_done, num_loops = time_training(model, train_dataset_tf, STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.concat([pd.read_parquet(x) for x in output_train_dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Time: 445.9770152568817\n",
      "Throughput: 439130.21590853843\n"
     ]
    }
   ],
   "source": [
    "log_textfile(logfilename, 'Training', 'w')\n",
    "log_textfile(logfilename, 'Time: ' + str(run_time), 'a')\n",
    "log_textfile(logfilename, 'Throughput: ' + str(df.shape[0]/run_time), 'a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
