{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# tools for data preproc/loading\n",
    "import torch\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize,  Categorify,  LogOp, FillMissing, Clip, get_embedding_sizes\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "from nvtabular.utils import device_mem_size\n",
    "\n",
    "# tools for training\n",
    "from fastai.basics import Learner\n",
    "from fastai.tabular.model import TabularModel\n",
    "from fastai.tabular.data import TabularDataLoaders\n",
    "from fastai.metrics import accuracy\n",
    "from fastai.callback.progress import ProgressCallback\n",
    "\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function\n",
    "\n",
    "def preproces_criteo():\n",
    "    fname = 'day_{}.parquet'\n",
    "    num_days = len([i for i in os.listdir(INPUT_DATA_DIR) if re.match(fname.format('[0-9]{1,2}'), i) is not None])\n",
    "    train_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in range(1)]\n",
    "    valid_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in [2]]\n",
    "    train_paths, valid_paths\n",
    "    \n",
    "    proc = nvt.Workflow(\n",
    "        cat_names=CATEGORICAL_COLUMNS,\n",
    "        cont_names=CONTINUOUS_COLUMNS,\n",
    "        label_name=LABEL_COLUMNS\n",
    "    )\n",
    "    \n",
    "    proc.add_cont_feature([FillMissing(), Clip(min_value=0), LogOp()])\n",
    "    proc.add_cont_preprocess(Normalize())\n",
    "    proc.add_cat_preprocess(Categorify(freq_threshold=15, out_path=OUTPUT_DATA_DIR))\n",
    "    \n",
    "    train_dataset = nvt.Dataset(train_paths, engine='parquet', part_mem_fraction=0.15)\n",
    "    valid_dataset = nvt.Dataset(valid_paths, engine='parquet', part_mem_fraction=0.15)\n",
    "    \n",
    "    os.system('rm -r ' + OUTPUT_DATA_DIR)\n",
    "    os.system('mkdir -p ' + output_train_dir)\n",
    "    os.system('mkdir -p ' + output_valid_dir)\n",
    "    \n",
    "    proc.apply(train_dataset, \n",
    "               shuffle=nvt.io.Shuffle.PER_PARTITION, \n",
    "               output_path=output_train_dir, \n",
    "               out_files_per_proc=20\n",
    "              )\n",
    "    \n",
    "    proc.apply(valid_dataset, \n",
    "               record_stats=False, \n",
    "               shuffle=nvt.io.Shuffle.PER_PARTITION, \n",
    "               output_path=output_valid_dir, \n",
    "               out_files_per_proc=20\n",
    "              )\n",
    "    \n",
    "    proc.save_stats(BASE_DIR + 'stats_wnd_workflow')\n",
    "\n",
    "def get_model(hidden_dims, inputs, features):\n",
    "    dense_layer = layers.DenseFeatures(features)\n",
    "    x = dense_layer(inputs)\n",
    "    \n",
    "    for hidden in hidden_dims:\n",
    "        x = tf.keras.layers.Dense(hidden, activation='relu')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    metrics = [tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\")]\n",
    "    model.compile('sgd', 'binary_crossentropy', metrics=metrics)\n",
    "    return(model)\n",
    "\n",
    "def time_only_dl(dl, num_steps):\n",
    "    start = time.time()\n",
    "    i = 0\n",
    "    j= 0\n",
    "    bl_done = False\n",
    "    while not(bl_done) and i<num_steps:\n",
    "        for _, batch in enumerate(dl):\n",
    "            if i == num_steps:\n",
    "                bl_done = True\n",
    "                break\n",
    "            i+=1\n",
    "        j+=1\n",
    "    end = time.time()\n",
    "    return(end-start, i, j)\n",
    "\n",
    "def time_training(model, train_dataset_tf, steps):\n",
    "    start = time.time()\n",
    "    history = model.fit(train_dataset_tf, epochs=1, steps_per_epoch=steps)\n",
    "    end = time.time()\n",
    "    return(end-start, steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2288"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which data loader to use\n",
    "DL_TYPE = 'NVTabular'               # 'NVTabular', TensorFlow'\n",
    "BENCHMARK_TYPE = 'convergence_val_loss'\n",
    "\n",
    "# define some information about where to get our data\n",
    "BASE_DIR = '/raid/data/criteo/'\n",
    "INPUT_DATA_DIR = os.environ.get('INPUT_DATA_DIR', BASE_DIR + 'input/')\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', BASE_DIR + 'output') # where we'll save our procesed data to\n",
    "TFRECORD_DIR = os.environ.get(\"TFRECORD_DIR\", BASE_DIR + 'tfrecords')\n",
    "TFRECORDS_TRAIN = os.path.join(TFRECORD_DIR, 'train', '*.tfrecords')\n",
    "TFRECORDS_VALID = os.path.join(TFRECORD_DIR, 'train', '*.tfrecords')\n",
    "\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 1024*64))\n",
    "EPOCHS = 2\n",
    "TRAIN_STEPS = 20\n",
    "_EXAMPLES_PER_RECORD = 20000000\n",
    "STEPS = int(150000000*1/BATCH_SIZE)\n",
    "\n",
    "HIDDEN_DIMS = [128, 128, 128, 128]\n",
    "PREPROCESS = False\n",
    "PREPROCESS_TF = False\n",
    "\n",
    "\n",
    "PARTS_PER_CHUNK = int(os.environ.get('PARTS_PER_CHUNK', 2))\n",
    "NUM_TRAIN_DAYS = 23 # number of days worth of data to use for training, the rest will be used for validation\n",
    "\n",
    "# define our dataset schema\n",
    "CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\n",
    "CATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS\n",
    "STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train/')\n",
    "output_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid/')\n",
    "tf_input_train_dir = output_train_dir + '*.parquet'\n",
    "tf_input_valid_dir = output_valid_dir + '*.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROCESS:\n",
    "    preproces_criteo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = nvt.Workflow(\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    label_name=LABEL_COLUMNS\n",
    ")\n",
    "proc.load_stats(BASE_DIR + 'stats_wnd_workflow')\n",
    "EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.feature_column import feature_column_v2 as fc\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "os.environ['TF_MEMORY_ALLOCATION'] = \"0.5\" # fraction of free memory\n",
    "from nvtabular.loader.tensorflow import KerasSequenceLoader, KerasSequenceValidater\n",
    "from nvtabular.framework_utils.tensorflow import layers\n",
    "from tensorflow.python.feature_column import feature_column_v2 as fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "features = []\n",
    "\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    inputs[col] =  tf.keras.Input(\n",
    "        name=col,\n",
    "        dtype=tf.int32,\n",
    "        shape=(1,)\n",
    "    )\n",
    "    features.append(\n",
    "        tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(\n",
    "                col, \n",
    "                EMBEDDING_TABLE_SHAPES[col][0]                    # Input dimension (vocab size)\n",
    "            ), EMBEDDING_TABLE_SHAPES[col][1]                     # Embedding output dimension\n",
    "        )\n",
    "    )\n",
    "for col in CONTINUOUS_COLUMNS:\n",
    "    inputs[col] =  tf.keras.Input(\n",
    "        name=col,\n",
    "        dtype=tf.float32,\n",
    "        shape=(1,)\n",
    "    )\n",
    "    features.append(\n",
    "        tf.feature_column.numeric_column(col, (1,))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tf = KerasSequenceLoader(\n",
    "    output_train_dir + '*.parquet', # you could also use a glob pattern\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=LABEL_COLUMNS,\n",
    "    cat_names=CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    engine='parquet',\n",
    "    shuffle=True,\n",
    "    buffer_size=0.06, # how many batches to load at once\n",
    "    parts_per_chunk=1\n",
    ")\n",
    "valid_dataset_tf = KerasSequenceLoader(\n",
    "    output_valid_dir + '*.parquet', # you could also use a glob pattern\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_names=LABEL_COLUMNS,\n",
    "    cat_names = CATEGORICAL_COLUMNS,\n",
    "    cont_names=CONTINUOUS_COLUMNS,\n",
    "    engine='parquet',\n",
    "    shuffle=False,\n",
    "    buffer_size=0.06,\n",
    "    parts_per_chunk=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVTabular'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DL_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasSequenceValidater2_ModelPrediction(tf.keras.callbacks.Callback):\n",
    "    # TODO: document\n",
    "    _supports_tf_logs = True\n",
    "\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        i = 0\n",
    "        start = time.time()\n",
    "        for X, y_true in self.dataloader:\n",
    "            i += 1\n",
    "            y_pred = self.model(X)\n",
    "\n",
    "            # TODO: how do we want to handle the multi-output case?\n",
    "            #for metric in self.model.metrics:\n",
    "            #    metric.update_state(y_true, y_pred)\n",
    "            if (i % 10)==0:\n",
    "                end = time.time()\n",
    "                print('Time 10 batches: ' + str(end-start))\n",
    "                start = time.time()\n",
    "                print(i)\n",
    "            if (i>100):\n",
    "                break\n",
    "        \n",
    "        #for metric in self.model.metrics:\n",
    "        #    logs[\"val_\" + metric.name] = metric.result().numpy()\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(HIDDEN_DIMS, inputs, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 10 batches: 5.90089750289917\n",
      "10\n",
      "Time 10 batches: 0.7925128936767578\n",
      "20\n",
      "Time 10 batches: 1.0462443828582764\n",
      "30\n",
      "Time 10 batches: 0.9086611270904541\n",
      "40\n",
      "Time 10 batches: 1.1971657276153564\n",
      "50\n",
      "Time 10 batches: 0.9518229961395264\n",
      "60\n",
      "Time 10 batches: 0.799907922744751\n",
      "70\n",
      "Time 10 batches: 0.45679450035095215\n",
      "80\n",
      "Time 10 batches: 0.9126536846160889\n",
      "90\n",
      "Time 10 batches: 0.9586730003356934\n",
      "100\n",
      "CPU times: user 17.1 s, sys: 6.55 s, total: 23.6 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Just executing the model prediction step outside validation callback\n",
    "i = 0\n",
    "start = time.time()\n",
    "for X, y_true in train_dataset_tf:\n",
    "    i += 1\n",
    "    y_pred = model(X)\n",
    "    if (i % 10)==0:\n",
    "        end = time.time()\n",
    "        print('Time 10 batches: ' + str(end-start))\n",
    "        start = time.time()\n",
    "        print(i)\n",
    "    if (i>100):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 10 batches: 4.833057165145874\n",
      "10\n",
      "Time 10 batches: 3.9489147663116455\n",
      "20\n",
      "Time 10 batches: 6.718460559844971\n",
      "30\n",
      "Time 10 batches: 7.785595417022705\n",
      "40\n",
      "Time 10 batches: 7.827616453170776\n",
      "50\n",
      "Time 10 batches: 4.010151147842407\n",
      "60\n",
      "Time 10 batches: 6.016873121261597\n",
      "70\n",
      "Time 10 batches: 6.478650093078613\n",
      "80\n",
      "Time 10 batches: 5.842178106307983\n",
      "90\n",
      "Time 10 batches: 5.280597925186157\n",
      "100\n",
      "CPU times: user 1min 7s, sys: 6.29 s, total: 1min 14s\n",
      "Wall time: 59.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Just executing the model prediction step outside validation callback\n",
    "i = 0\n",
    "start = time.time()\n",
    "for X, y_true in valid_dataset_tf:\n",
    "    i += 1\n",
    "    y_pred = model(X)\n",
    "    if (i % 10)==0:\n",
    "        end = time.time()\n",
    "        print('Time 10 batches: ' + str(end-start))\n",
    "        start = time.time()\n",
    "        print(i)\n",
    "    if (i>100):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 10 batches: 0.019385337829589844\n",
      "10\n",
      "Time 10 batches: 9.036064147949219e-05\n",
      "20\n",
      "Time 10 batches: 0.00010704994201660156\n",
      "30\n",
      "Time 10 batches: 8.511543273925781e-05\n",
      "40\n",
      "Time 10 batches: 0.0628824234008789\n",
      "50\n",
      "Time 10 batches: 0.015411138534545898\n",
      "60\n",
      "Time 10 batches: 0.0001266002655029297\n",
      "70\n",
      "Time 10 batches: 3.266334533691406e-05\n",
      "80\n",
      "Time 10 batches: 3.1948089599609375e-05\n",
      "90\n",
      "Time 10 batches: 3.1948089599609375e-05\n",
      "100\n",
      "CPU times: user 100 ms, sys: 48.6 ms, total: 149 ms\n",
      "Wall time: 119 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Just executing the model prediction step outside validation callback\n",
    "i = 0\n",
    "start = time.time()\n",
    "for X, y_true in valid_dataset_tf:\n",
    "    i += 1\n",
    "    #y_pred = model(X)\n",
    "    if (i % 10)==0:\n",
    "        end = time.time()\n",
    "        print('Time 10 batches: ' + str(end-start))\n",
    "        start = time.time()\n",
    "        print(i)\n",
    "    if (i>100):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 0.6609 - auroc: 0.4719- ETA: 3s - loss: 0.7251 - Time 10 batches: 9.153592348098755\n",
      "10\n",
      "Time 10 batches: 7.0142738819122314\n",
      "20\n",
      "Time 10 batches: 5.858387470245361\n",
      "30\n",
      "Time 10 batches: 4.22637677192688\n",
      "40\n",
      "Time 10 batches: 7.8126726150512695\n",
      "50\n",
      "Time 10 batches: 6.899638652801514\n",
      "60\n",
      "Time 10 batches: 7.092344522476196\n",
      "70\n",
      "Time 10 batches: 7.813392400741577\n",
      "80\n",
      "Time 10 batches: 6.825751304626465\n",
      "90\n",
      "Time 10 batches: 8.5209801197052\n",
      "100\n",
      "20/20 [==============================] - 74s 4s/step - loss: 0.6609 - auroc: 0.4719\n"
     ]
    }
   ],
   "source": [
    "validation_callback = KerasSequenceValidater2_ModelPrediction(valid_dataset_tf)\n",
    "history = model.fit(train_dataset_tf, \n",
    "                    epochs=1, \n",
    "                    steps_per_epoch=20,\n",
    "                    callbacks=[validation_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasSequenceValidater2_noModelPrediction(tf.keras.callbacks.Callback):\n",
    "    # TODO: document\n",
    "    _supports_tf_logs = True\n",
    "\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        i = 0\n",
    "        start = time.time()\n",
    "        for X, y_true in self.dataloader:\n",
    "            i += 1\n",
    "            #y_pred = self.model(X)\n",
    "\n",
    "            # TODO: how do we want to handle the multi-output case?\n",
    "            #for metric in self.model.metrics:\n",
    "            #    metric.update_state(y_true, y_pred)\n",
    "            if (i % 10)==0:\n",
    "                end = time.time()\n",
    "                print('Time 10 batches: ' + str(end-start))\n",
    "                start = time.time()\n",
    "                print(i)\n",
    "            if (i>100):\n",
    "                break\n",
    "            \n",
    "        #for metric in self.model.metrics:\n",
    "        #    logs[\"val_\" + metric.name] = metric.result().numpy()\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 0.5204 - auroc: 0.4343Time 10 batches: 8.563394784927368\n",
      "10\n",
      "Time 10 batches: 8.535385131835938e-05\n",
      "20\n",
      "Time 10 batches: 8.034706115722656e-05\n",
      "30\n",
      "Time 10 batches: 8.249282836914062e-05\n",
      "40\n",
      "Time 10 batches: 8.130073547363281e-05\n",
      "50\n",
      "Time 10 batches: 8.106231689453125e-05\n",
      "60\n",
      "Time 10 batches: 8.153915405273438e-05\n",
      "70\n",
      "Time 10 batches: 0.030339717864990234\n",
      "80\n",
      "Time 10 batches: 0.010444879531860352\n",
      "90\n",
      "Time 10 batches: 0.010492086410522461\n",
      "100\n",
      "20/20 [==============================] - 16s 783ms/step - loss: 0.5204 - auroc: 0.4343\n"
     ]
    }
   ],
   "source": [
    "validation_callback = KerasSequenceValidater2_noModelPrediction(valid_dataset_tf)\n",
    "history = model.fit(train_dataset_tf, \n",
    "                    epochs=1, \n",
    "                    steps_per_epoch=20,\n",
    "                    callbacks=[validation_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
