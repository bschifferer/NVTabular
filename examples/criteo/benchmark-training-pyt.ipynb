{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Benchmark NVTabular data loader\n",
    "We are interested to benchmark the NVTabular data loader and compare its performance to the TensorFlow \"native\" data loader based on tf.records. In [benchmark-preprocess.ipynb](???), we preprocess the dataset, ready to use for NVTabular data loader (parquet) and TensorFlow native data loader (tf.records). In this notebook, we will train a neural network in TensorFlow using either data loader and measure the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpustat in /opt/conda/envs/rapids/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/rapids/lib/python3.7/site-packages (4.31.1)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from gpustat) (7.352.0)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from gpustat) (1.15.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/rapids/lib/python3.7/site-packages (from gpustat) (5.7.2)\n",
      "Requirement already satisfied: blessings>=1.6 in /opt/conda/envs/rapids/lib/python3.7/site-packages (from gpustat) (1.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install gpustat tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run single GPU version and set only one visible device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gc\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "\n",
    "import nvtabular as nvt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from fastai.tabular.data import TabularDataLoaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the CustomDataset, which is used for the native PyTorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  \"\"\"Simple dataset class for dataloader\"\"\"\n",
    "  def __init__(self, X, cont_names, cat_names, label_name):\n",
    "    \"\"\"Initialize the CustomDataset\"\"\"\n",
    "    self.X_cont = X[cont_names].values\n",
    "    self.X_cat = X[cat_names].values\n",
    "    self.y = X[label_name].values\n",
    "    \n",
    "  def __len__(self):\n",
    "    \"\"\"Return the total length of the dataset\"\"\"\n",
    "    dataset_size = self.X_cat.shape[0]\n",
    "    return dataset_size\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"Return the batch given the indices\"\"\"\n",
    "    return (self.X_cat[idx].astype(np.int64), \n",
    "            self.X_cont[idx].astype(np.float32), \n",
    "            self.y[idx].astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple feed forward neural network with embedding layers for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "\n",
    "from nvtabular.framework_utils.torch.utils import process_epoch\n",
    "\n",
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(self, cat_names, cat_stats):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "        emb_list = []\n",
    "        size = 0\n",
    "        for i, emb in enumerate(cat_names):\n",
    "            emb_size = cat_stats[emb][1]\n",
    "            emb_list.append(nn.Embedding(cat_stats[emb][0], emb_size))\n",
    "            size += emb_size\n",
    "        self.emb = nn.ModuleList(emb_list)\n",
    "        self.out_size = size\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = [e(X[:,i]) for i, e in enumerate(self.emb)]\n",
    "        return(x)\n",
    "\n",
    "class MLPTower(nn.Module):\n",
    "    def __init__(self, in_size, out_size, hidden_layers):\n",
    "        super(MLPTower, self).__init__()\n",
    "        layers = []\n",
    "        for i, tmp_out_size in enumerate(hidden_layers):\n",
    "            layers.append(nn.Linear(in_size, tmp_out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_size = tmp_out_size\n",
    "        layers.append(nn.Linear(in_size, out_size))\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.hidden(X)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 cont_names, \n",
    "                 cat_names, \n",
    "                 cat_stats,\n",
    "                 hidden_layers, \n",
    "                 out_size=1,\n",
    "                ):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.embblock = EmbeddingBlock(cat_names, cat_stats)\n",
    "        \n",
    "        mlp_input_size = self.embblock.out_size+len(cont_names)\n",
    "        self.topmlp = MLPTower(mlp_input_size, \n",
    "                               out_size=out_size, \n",
    "                               hidden_layers=hidden_layers\n",
    "                              )\n",
    "        \n",
    "    def forward(self, X_cat, X_num):\n",
    "        x_embblock = self.embblock(X_cat)\n",
    "        x_deep = torch.cat(x_embblock + [X_num], axis=1)\n",
    "        x_deep = self.topmlp(x_deep)\n",
    "        x_out = x_deep\n",
    "        return(torch.squeeze(x_out))\n",
    "    \n",
    "\n",
    "def process_epoch(\n",
    "    dataloader,\n",
    "    model,\n",
    "    train=False,\n",
    "    optimizer=None,\n",
    "    loss_func=torch.nn.MSELoss(),\n",
    "    transform=None,\n",
    "    amp=True,\n",
    "    device=None,\n",
    "    steps=None\n",
    "):\n",
    "    \"\"\"\n",
    "    The controlling function that loads data supplied via a dataloader to a model. Can be redefined\n",
    "    based on parameters.\n",
    "    Parameters\n",
    "    -----------\n",
    "    dataloader : iterator\n",
    "        Iterator that contains the dataset to be submitted to the model.\n",
    "    model : torch.nn.Module\n",
    "        Pytorch model to run data through.\n",
    "    train : bool\n",
    "        Indicate whether dataloader contains training set.\n",
    "    optimizer : object\n",
    "        Optimizer to run in conjunction with model.\n",
    "    loss_func : function\n",
    "        Loss function to use, default is MSELoss.\n",
    "    \"\"\"\n",
    "    n=0\n",
    "    model.train(mode=train)\n",
    "    with torch.set_grad_enabled(train):\n",
    "        y_list, y_pred_list = [], []\n",
    "        for idx, batch in enumerate(iter(dataloader)):\n",
    "            n+=batch[0].shape[0]\n",
    "            if transform:\n",
    "                x_cat, x_cont, y = transform(batch)\n",
    "            else:\n",
    "                x_cat, x_cont, y = batch\n",
    "            if device:\n",
    "                x_cat = x_cat.to(device)\n",
    "                x_cont = x_cont.to(device)\n",
    "                y = y.to(device)\n",
    "            y = torch.squeeze(y).float()\n",
    "            #y_list.append(y.detach())\n",
    "            # maybe autocast goes here?\n",
    "            if amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    y_pred = model(x_cat, x_cont)\n",
    "                    #y_pred_list.append(y_pred.detach())\n",
    "                    loss = loss_func(y_pred, y)\n",
    "            else:\n",
    "                y_pred = model(x_cat, x_cont)\n",
    "                #y_pred_list.append(y_pred.detach())\n",
    "                loss = loss_func(y_pred, y)\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if steps:\n",
    "                if (idx+1)>=steps:\n",
    "                    break\n",
    "    #y = torch.cat(y_list)\n",
    "    #y_pred = torch.cat(y_pred_list)\n",
    "    epoch_loss = loss_func(y_pred, y).item()\n",
    "    return epoch_loss, 0, 0, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define multiple helper functions.<br><br>\n",
    "*get_dataloader* returns the NVTabular data loader, PyTorch or FastAI data loader, depending on dl_type<br>\n",
    "*log_textfile* stores output to a textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function\n",
    "def get_dataloader(dl_type='NVTabular'):\n",
    "    if dl_type=='NVTabular':\n",
    "        train_dataset = TorchAsyncItr(nvt.Dataset(train_files), \n",
    "                                      batch_size=BATCH_SIZE, \n",
    "                                      cats=CATEGORICAL_COLUMNS, \n",
    "                                      conts=CONTINUOUS_COLUMNS, \n",
    "                                      labels=LABEL_COLUMNS\n",
    "                                     )\n",
    "        train_loader = DLDataLoader(train_dataset, \n",
    "                                    batch_size=None,\n",
    "                                    pin_memory=False, \n",
    "                                    num_workers=0\n",
    "                                   )\n",
    "        valid_dataset = TorchAsyncItr(nvt.Dataset(valid_files), \n",
    "                                      batch_size=BATCH_SIZE, \n",
    "                                      cats=CATEGORICAL_COLUMNS, \n",
    "                                      conts=CONTINUOUS_COLUMNS, \n",
    "                                      labels=LABEL_COLUMNS)\n",
    "        valid_loader = DLDataLoader(valid_dataset, \n",
    "                                    batch_size=None,\n",
    "                                    pin_memory=False, \n",
    "                                    num_workers=0)\n",
    "    if dl_type=='PyTorch':\n",
    "        pd_train = pd.concat([pd.read_parquet(x) for x in train_files])\n",
    "        #pd_valid = pd.concat([pd.read_parquet(x) for x in valid_files])\n",
    "        train_dataset = CustomDataset(pd_train, \n",
    "                                      cat_names=sorted(CATEGORICAL_COLUMNS), \n",
    "                                      cont_names=CONTINUOUS_COLUMNS, \n",
    "                                      label_name=LABEL_COLUMNS)\n",
    "        #valid_dataset = CustomDataset(pd_valid, \n",
    "        #                              cat_names=sorted(CATEGORICAL_COLUMNS), \n",
    "        #                              cont_names=CONTINUOUS_COLUMNS, \n",
    "        #                              label_name=LABEL_COLUMNS)\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size=BATCH_SIZE, \n",
    "                                  shuffle=True, \n",
    "                                  num_workers=16)\n",
    "        #valid_loader = DataLoader(valid_dataset, \n",
    "        #                          batch_size=BATCH_SIZE, \n",
    "        #                          shuffle=False,\n",
    "        #                          num_workers=16)\n",
    "        valid_loader = None\n",
    "    if dl_type=='FastAI':\n",
    "        df_valid = pd.concat([pd.read_parquet(x) for x in valid_files[0:1]])\n",
    "        df_train = pd.concat([pd.read_parquet(x) for x in train_files])\n",
    "        idx_train = df_train.shape[0]\n",
    "        df = pd.concat([df_train, df_valid])\n",
    "        del df_train, df_valid\n",
    "        gc.collect()\n",
    "        dl = TabularDataLoaders.from_df(df, \n",
    "                                cat_names=sorted(CATEGORICAL_COLUMNS), \n",
    "                                cont_names=CONTINUOUS_COLUMNS,\n",
    "                                y_names=LABEL_COLUMNS[0],\n",
    "                                valid_idx=list(range(idx_train,df.shape[0])),\n",
    "                                bs=BATCH_SIZE,\n",
    "                                val_bs=BATCH_SIZE,\n",
    "                                device=todevice\n",
    "                               )\n",
    "        train_loader, valid_loader = dl.loaders\n",
    "    return(train_loader, valid_loader)\n",
    "\n",
    "def log_textfile(filename, text, mode):\n",
    "    print(text)\n",
    "    f = open(filename, mode)\n",
    "    f.write(str(text) + str('\\n'))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we define functions to measure the performance.<br><br>\n",
    "*time_only_dl* measures the time for just iterating through the dataset for 1 epoch WITHOUT training a model<br>\n",
    "*time_training* measures the time for training a model for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_only_dl(dl):\n",
    "    start = time()\n",
    "    i = 0\n",
    "    n = 0\n",
    "    for _, batch in enumerate(dl):\n",
    "        i+=1\n",
    "        n+=batch[0].shape[0]\n",
    "    end = time()\n",
    "    return(end-start, i, n)\n",
    "\n",
    "def time_training(model, train_dataset, optimizer, amp=True, device=None):\n",
    "    start = time()\n",
    "    train_loss, y_pred, y, n = process_epoch(train_dataset, \n",
    "                                             model, \n",
    "                                             train=True,\n",
    "                                             loss_func=torch.nn.BCEWithLogitsLoss(),\n",
    "                                             optimizer=optimizer,\n",
    "                                             amp=amp,\n",
    "                                             device=device)\n",
    "    end = time()\n",
    "    return(end-start, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define which benchmark, we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMP = False\n",
    "DL_TYPES = ['NVTabular', 'PyTorch', 'FastAI']\n",
    "BENCHMARK_TYPES = ['time_only_dl', 'time_training', 'convergence_training_loss', 'convergence_val_loss']\n",
    "DL_TYPE = 'NVTabular'\n",
    "BENCHMARK_TYPE = 'time_training'\n",
    "CPU = False\n",
    "\n",
    "if DL_TYPE not in DL_TYPES:\n",
    "    raise ValueError(DL_TYPE + ' is not supported.  Choose from ' + str(DL_TYPES))\n",
    "    \n",
    "if BENCHMARK_TYPE not in BENCHMARK_TYPES:\n",
    "    raise ValueError(BENCHMARK_TYPE + ' is not supported. Choose from ' + str(BENCHMARK_TYPES))\n",
    "\n",
    "if CPU:\n",
    "    todevice = \"cpu\"\n",
    "else:\n",
    "    todevice = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the inpurt directory for the parquet and tf.records file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some information about where to get our data\n",
    "OUTPUT_DIR = '/raid/data/criteo/'\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', OUTPUT_DIR + 'output') # where we'll save our procesed data to\n",
    "\n",
    "output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train/')\n",
    "output_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid/')\n",
    "\n",
    "train_files = glob.glob(output_train_dir + '*.parquet')\n",
    "valid_files = glob.glob(output_valid_dir + '*.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some hyperparameters and network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for training the deep learning model\n",
    "BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 1024*64))      \n",
    "# Number of epochs (only for convergence_val_loss)\n",
    "EPOCHS = 5\n",
    "# Number of steps in training to collect train_loss (only for convergence_training_loss)\n",
    "TRAIN_STEPS = 20\n",
    "# Max. number of steps per epoch (tf.records allows only full batches)\n",
    "STEPS = int(150000000/BATCH_SIZE)\n",
    "# Number of units in hidden layer - length is number of hidden layers\n",
    "HIDDEN_DIMS = [1024, 1024, 1024, 1024]\n",
    "# Number of parts using in shuffling of NVTabular data loader\n",
    "PARTS_PER_CHUNK = int(os.environ.get('PARTS_PER_CHUNK', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the saved NVTabular workflow to extract the data schema and some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = nvt.Workflow(\n",
    "    cat_names=[],\n",
    "    cont_names=[],\n",
    "    label_name=[]\n",
    ")\n",
    "proc.load_stats(OUTPUT_DATA_DIR + '/stats_and_workflow')\n",
    "# for col in proc.stats[\"categories\"]:\n",
    "#     proc.stats[\"categories\"][col] = proc.stats[\"categories\"][col].replace('/raid/data/criteo/', OUTPUT_DIR)\n",
    "EMBEDDING_TABLE_SHAPES = nvt.ops.get_embedding_sizes(proc)\n",
    "\n",
    "CATEGORICAL_COLUMNS = proc.columns_ctx['categorical']['base']\n",
    "CONTINUOUS_COLUMNS = proc.columns_ctx['continuous']['base']\n",
    "LABEL_COLUMNS = proc.columns_ctx['label']['base']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(\n",
    "    cont_names=CONTINUOUS_COLUMNS, \n",
    "    cat_names=sorted(CATEGORICAL_COLUMNS), \n",
    "    cat_stats=EMBEDDING_TABLE_SHAPES,\n",
    "    hidden_layers=HIDDEN_DIMS\n",
    ").to(todevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (embblock): EmbeddingBlock(\n",
       "    (emb): ModuleList(\n",
       "      (0): Embedding(381808, 16)\n",
       "      (1): Embedding(341642, 16)\n",
       "      (2): Embedding(112151, 16)\n",
       "      (3): Embedding(94957, 16)\n",
       "      (4): Embedding(11, 6)\n",
       "      (5): Embedding(2188, 16)\n",
       "      (6): Embedding(8399, 16)\n",
       "      (7): Embedding(61, 16)\n",
       "      (8): Embedding(4, 3)\n",
       "      (9): Embedding(949, 16)\n",
       "      (10): Embedding(15, 7)\n",
       "      (11): Embedding(22456, 16)\n",
       "      (12): Embedding(382633, 16)\n",
       "      (13): Embedding(246818, 16)\n",
       "      (14): Embedding(370704, 16)\n",
       "      (15): Embedding(92823, 16)\n",
       "      (16): Embedding(9773, 16)\n",
       "      (17): Embedding(78, 16)\n",
       "      (18): Embedding(34, 12)\n",
       "      (19): Embedding(14763, 16)\n",
       "      (20): Embedding(7118, 16)\n",
       "      (21): Embedding(19308, 16)\n",
       "      (22): Embedding(4, 3)\n",
       "      (23): Embedding(6443, 16)\n",
       "      (24): Embedding(1259, 16)\n",
       "      (25): Embedding(54, 15)\n",
       "    )\n",
       "  )\n",
       "  (topmlp): MLPTower(\n",
       "    (hidden): Sequential(\n",
       "      (0): Linear(in_features=379, out_features=1024, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get our dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = get_dataloader(DL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update some parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AMP:\n",
    "    amp_postfix = 'amp'\n",
    "else:\n",
    "    amp_postfix = 'noamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain combinations are available by default and we do not specfic move the data to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DL_TYPE=='NVTabular' and todevice=='cuda:0':\n",
    "    todevice=None\n",
    "if DL_TYPE=='FastAI' and todevice=='cuda:0':\n",
    "    todevice=None\n",
    "if DL_TYPE=='FastAI' and todevice=='cpu':\n",
    "    todevice=None\n",
    "if DL_TYPE=='PyTorch' and todevice=='cpu':\n",
    "    todevice=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only Data Loader\n",
      "Time: 11.410465478897095\n",
      "Throughput: 17163364.926890746\n"
     ]
    }
   ],
   "source": [
    "logfilename = DL_TYPE + '_cpu' + str(CPU) + '_' + BENCHMARK_TYPE + '_' + amp_postfix + '.log'\n",
    "if BENCHMARK_TYPE=='time_only_dl':\n",
    "    os.system('gpustat --watch >> ' + DL_TYPE + '_cpu' + str(CPU) + '_only_dl_' + amp_postfix + '.json &')\n",
    "    run_time, n_batches, n_samples = time_only_dl(train_dataset)\n",
    "    log_textfile(logfilename, 'Only Data Loader', 'w')\n",
    "    log_textfile(logfilename, 'Time: ' + str(run_time), 'a')\n",
    "    log_textfile(logfilename, 'Throughput: ' + str(n_samples/run_time), 'a')\n",
    "    os.system('pkill -f \"gpustat\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Time: 473.4369068145752\n",
      "Throughput: 413660.1523478246\n"
     ]
    }
   ],
   "source": [
    "if BENCHMARK_TYPE=='time_training':\n",
    "    os.system('gpustat --watch >> ' + DL_TYPE + '_cpu' + str(CPU) + '_training_' + amp_postfix + '.json &')\n",
    "    run_time, n_samples = time_training(model, train_dataset, optimizer, amp=AMP, device=todevice)\n",
    "    log_textfile(logfilename, 'Training', 'w')\n",
    "    log_textfile(logfilename, 'Len: ' + str(len(train_dataset)), 'a')\n",
    "    log_textfile(logfilename, 'Samples: ' + str(n_samples), 'a')\n",
    "    log_textfile(logfilename, 'Time: ' + str(run_time), 'a')\n",
    "    log_textfile(logfilename, 'Throughput: ' + str(n_samples/run_time), 'a')\n",
    "    os.system('pkill -f \"gpustat\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BENCHMARK_TYPE=='convergence_training_loss':\n",
    "    history =[]\n",
    "    for i in range(EPOCHS):\n",
    "        epoch_loss, y_pred, y, n = process_epoch(train_dataset, \n",
    "                                                 model, \n",
    "                                                 train=True,\n",
    "                                                 loss_func=torch.nn.BCEWithLogitsLoss(),\n",
    "                                                 optimizer=optimizer, \n",
    "                                                 steps=TRAIN_STEPS, \n",
    "                                                 device=todevice)\n",
    "        history.append({'step': i, 'train_loss': epoch_loss})\n",
    "    pickle.dump(history, open(logfilename.replace('.log', '.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BENCHMARK_TYPE=='convergence_val_loss':\n",
    "    history =[]\n",
    "    for i in range(EPOCHS):\n",
    "        train_loss, y_pred, y, n = process_epoch(train_dataset, \n",
    "                                                 model, \n",
    "                                                 train=True,\n",
    "                                                 loss_func=torch.nn.BCEWithLogitsLoss(),\n",
    "                                                 optimizer=optimizer,  \n",
    "                                                 device=todevice)\n",
    "        valid_loss, y_pred, y, n = process_epoch(valid_dataset, \n",
    "                                                 model, \n",
    "                                                 train=False,\n",
    "                                                 loss_func=torch.nn.BCEWithLogitsLoss(), \n",
    "                                                 device=todevice)\n",
    "        history.append({'step': i, 'train_loss': train_loss, 'valid_loss': valid_loss})\n",
    "    pickle.dump(history, open(logfilename.replace('.log', '.pickle'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
