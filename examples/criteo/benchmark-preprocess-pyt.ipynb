{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Preprocessing the Criteo Dataset\n",
    "We are interested to benchmark the NVTabular data loader and compare its performance to the PyTorch \"native\" data loader and FastAI. We need to preprocess the dataset with NVTabular to normalize continuous features and categorify categorical ones.<br><br>\n",
    "The input for this notebook is based on [optimize_criteo.ipynb](https://github.com/NVIDIA/NVTabular/blob/main/examples/optimize_criteo.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "# tools for data preproc/loading\n",
    "import torch\n",
    "import rmm\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import Normalize,  Categorify,  LogOp, FillMissing, Clip, get_embedding_sizes\n",
    "from nvtabular.loader.torch import TorchAsyncItr, DLDataLoader\n",
    "from nvtabular.utils import device_mem_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a helper function.<br><br>\n",
    "*preproces_criteo* defines a NVTabular workflow to preprocess the data. It fills missing values, clip them, apply the logarithm function. Finally, the continuous features are normalized and categorical features are categorify. For more details, take a look on the [Criteo example](https://github.com/NVIDIA/NVTabular/blob/main/examples/criteo-example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function\n",
    "\n",
    "def preproces_criteo():\n",
    "    fname = 'day_{}.parquet'\n",
    "    num_days = len([i for i in os.listdir(INPUT_DATA_DIR) if re.match(fname.format('[0-9]{1,2}'), i) is not None])\n",
    "    train_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in range(1)]\n",
    "    valid_paths = [os.path.join(INPUT_DATA_DIR, fname.format(day)) for day in [2]]\n",
    "    train_paths, valid_paths\n",
    "    \n",
    "    proc = nvt.Workflow(\n",
    "        cat_names=CATEGORICAL_COLUMNS,\n",
    "        cont_names=CONTINUOUS_COLUMNS,\n",
    "        label_name=LABEL_COLUMNS\n",
    "    )\n",
    "    \n",
    "    proc.add_cont_feature([FillMissing(), Clip(min_value=0), LogOp()])\n",
    "    proc.add_cont_preprocess(Normalize())\n",
    "    proc.add_cat_preprocess(Categorify(freq_threshold=15, out_path=OUTPUT_DATA_DIR))\n",
    "    \n",
    "    train_dataset = nvt.Dataset(train_paths, engine='parquet', part_mem_fraction=0.15)\n",
    "    valid_dataset = nvt.Dataset(valid_paths, engine='parquet', part_mem_fraction=0.15)\n",
    "    \n",
    "    os.system('rm -r ' + OUTPUT_DATA_DIR)\n",
    "    os.system('mkdir -p ' + output_train_dir)\n",
    "    os.system('mkdir -p ' + output_valid_dir)\n",
    "    \n",
    "    proc.apply(train_dataset, \n",
    "               shuffle=nvt.io.Shuffle.PER_PARTITION, \n",
    "               output_path=output_train_dir, \n",
    "               out_files_per_proc=20\n",
    "              )\n",
    "    \n",
    "    proc.apply(valid_dataset, \n",
    "               record_stats=False, \n",
    "               shuffle=nvt.io.Shuffle.PER_PARTITION, \n",
    "               output_path=output_valid_dir, \n",
    "               out_files_per_proc=20\n",
    "              )\n",
    "    \n",
    "    proc.save_stats(OUTPUT_DATA_DIR + '/stats_and_workflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the directory stucture. The base directory and input and output directories for .parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some information about where to get our data\n",
    "INPUT_DIR = '/raid/data/criteo/input/'\n",
    "OUTPUT_DIR = '/raid/data/criteo/'\n",
    "INPUT_DATA_DIR = INPUT_DIR\n",
    "OUTPUT_DATA_DIR = os.environ.get('OUTPUT_DATA_DIR', OUTPUT_DIR + 'output') # where we'll save our procesed data to\n",
    "\n",
    "output_train_dir = os.path.join(OUTPUT_DATA_DIR, 'train/')\n",
    "output_valid_dir = os.path.join(OUTPUT_DATA_DIR, 'valid/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the data schema, which column names are continouos, categorical and label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset schema\n",
    "CONTINUOUS_COLUMNS = ['I' + str(x) for x in range(1,14)]\n",
    "CATEGORICAL_COLUMNS =  ['C' + str(x) for x in range(1,27)]\n",
    "LABEL_COLUMNS = ['label']\n",
    "COLUMNS = CONTINUOUS_COLUMNS + CATEGORICAL_COLUMNS + LABEL_COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the NVTabular workflow to preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 1min 4s, total: 2min 32s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "preproces_criteo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
